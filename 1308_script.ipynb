{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete. Saved to extracted_table.xlsx\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('sample_image.png')\n",
    "\n",
    "# Preprocess the image\n",
    "def preprocess_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Sharpen the image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5,-1], [0, -1, 0]])\n",
    "    sharp = cv2.filter2D(gray, -1, kernel)\n",
    "    \n",
    "    # Resize the image\n",
    "    height, width = sharp.shape\n",
    "    sharp = cv2.resize(sharp, (width*2, height*2))\n",
    "    \n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(sharp, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                   cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    return thresh\n",
    "\n",
    "# Preprocess the image\n",
    "thresh = preprocess_image(image)\n",
    "\n",
    "# Use Tesseract to perform OCR with custom configurations\n",
    "custom_config = r'--oem 3 --psm 6'\n",
    "data = pytesseract.image_to_data(thresh, output_type=Output.DICT, config=custom_config)\n",
    "\n",
    "# Extract the OCR results into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter out non-table data\n",
    "table_data = df[df['text'].str.strip().astype(bool)]\n",
    "\n",
    "# Initialize variables to store row data\n",
    "rows = []\n",
    "current_row = []\n",
    "current_line_num = table_data['line_num'].min()\n",
    "\n",
    "# Iterate through the table data\n",
    "for i, row in table_data.iterrows():\n",
    "    if row['line_num'] != current_line_num:\n",
    "        if current_row:\n",
    "            rows.append(' '.join(current_row))\n",
    "        current_row = []\n",
    "        current_line_num = row['line_num']\n",
    "    \n",
    "    # Concatenate text within the same line\n",
    "    current_row.append(row['text'])\n",
    "\n",
    "# Append the last row\n",
    "if current_row:\n",
    "    rows.append(' '.join(current_row))\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "df_table = pd.DataFrame(rows)\n",
    "\n",
    "# Ensure DataFrame has columns and rename them if needed\n",
    "# Assuming the first row of `rows` is the header\n",
    "if not df_table.empty:\n",
    "    header = df_table.iloc[0]\n",
    "    df_table = df_table[1:]\n",
    "    df_table.columns = header\n",
    "\n",
    "# Save to Excel\n",
    "df_table.to_excel(\"extracted_table.xlsx\", index=False)\n",
    "\n",
    "print(\"Data extraction complete. Saved to extracted_table.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete. Saved to extracted_table.xlsx\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('scrap1.jpg')\n",
    "\n",
    "# Preprocess the image\n",
    "def preprocess_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Sharpen the image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5,-1], [0, -1, 0]])\n",
    "    sharp = cv2.filter2D(gray, -1, kernel)\n",
    "    \n",
    "    # Resize the image\n",
    "    height, width = sharp.shape\n",
    "    sharp = cv2.resize(sharp, (width*2, height*2))\n",
    "    \n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(sharp, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                   cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    return thresh\n",
    "\n",
    "# Preprocess the image\n",
    "thresh = preprocess_image(image)\n",
    "\n",
    "# Use Tesseract to perform OCR with custom configurations\n",
    "custom_config = r'--oem 3 --psm 6'\n",
    "data = pytesseract.image_to_data(thresh, output_type=Output.DICT, config=custom_config)\n",
    "\n",
    "# Extract the OCR results into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter out non-table data\n",
    "table_data = df[df['text'].str.strip().astype(bool)]\n",
    "\n",
    "# Initialize variables to store row data\n",
    "rows = []\n",
    "current_row = []\n",
    "current_line_num = table_data['line_num'].min()\n",
    "\n",
    "# Iterate through the table data\n",
    "for i, row in table_data.iterrows():\n",
    "    if row['line_num'] != current_line_num:\n",
    "        if current_row:\n",
    "            rows.append(' '.join(current_row))\n",
    "        current_row = []\n",
    "        current_line_num = row['line_num']\n",
    "    \n",
    "    # Concatenate text within the same line\n",
    "    current_row.append(row['text'])\n",
    "\n",
    "# Append the last row\n",
    "if current_row:\n",
    "    rows.append(' '.join(current_row))\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "df_table = pd.DataFrame(rows)\n",
    "\n",
    "# Check if the DataFrame is not empty and has more than one row\n",
    "if not df_table.empty and len(df_table) > 1:\n",
    "    # Ensure DataFrame has columns and rename them if needed\n",
    "    # Assuming the first row of `rows` is the header\n",
    "    header = df_table.iloc[0]\n",
    "    df_table = df_table[1:]\n",
    "    df_table.columns = header\n",
    "else:\n",
    "    print(\"No valid data found to form a table.\")\n",
    "\n",
    "# Save to Excel\n",
    "df_table.to_excel(\"extracted_table.xlsx\", index=False)\n",
    "\n",
    "print(\"Data extraction complete. Saved to extracted_table.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete. Saved to extracted_table.xlsx\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image_path = 'sample_image.png'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print(f\"Error: Unable to load image at path {image_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Preprocess the image\n",
    "def preprocess_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Sharpen the image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharp = cv2.filter2D(gray, -1, kernel)\n",
    "    \n",
    "    # Denoise the image\n",
    "    denoised = cv2.fastNlMeansDenoising(sharp, h=30)\n",
    "    \n",
    "    # Resize the image\n",
    "    height, width = denoised.shape\n",
    "    resized = cv2.resize(denoised, (width * 3, height * 3))\n",
    "    \n",
    "    # Apply both adaptive and global thresholding\n",
    "    adaptive_thresh = cv2.adaptiveThreshold(resized, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                            cv2.THRESH_BINARY, 11, 2)\n",
    "    _, global_thresh = cv2.threshold(resized, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Combine both thresholding results\n",
    "    combined_thresh = cv2.bitwise_or(adaptive_thresh, global_thresh)\n",
    "    \n",
    "    return combined_thresh\n",
    "\n",
    "# Preprocess the image\n",
    "thresh = preprocess_image(image)\n",
    "\n",
    "# Use Tesseract to perform OCR with custom configurations\n",
    "custom_config = r'--oem 3 --psm 6'\n",
    "data = pytesseract.image_to_data(thresh, output_type=Output.DICT, config=custom_config)\n",
    "\n",
    "# Extract the OCR results into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter out non-table data\n",
    "table_data = df[df['text'].str.strip().astype(bool)]\n",
    "\n",
    "# Initialize variables to store row data\n",
    "rows = []\n",
    "current_row = []\n",
    "current_line_num = table_data['line_num'].min()\n",
    "\n",
    "# Iterate through the table data\n",
    "for i, row in table_data.iterrows():\n",
    "    if row['line_num'] != current_line_num:\n",
    "        if current_row:\n",
    "            rows.append(' '.join(current_row))\n",
    "        current_row = []\n",
    "        current_line_num = row['line_num']\n",
    "    \n",
    "    # Concatenate text within the same line\n",
    "    current_row.append(row['text'])\n",
    "\n",
    "# Append the last row\n",
    "if current_row:\n",
    "    rows.append(' '.join(current_row))\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "df_table = pd.DataFrame(rows)\n",
    "\n",
    "# Ensure DataFrame has columns and rename them if needed\n",
    "# Assuming the first row of `rows` is the header\n",
    "if not df_table.empty:\n",
    "    header = df_table.iloc[0]\n",
    "    df_table = df_table[1:]\n",
    "    df_table.columns = header\n",
    "\n",
    "# Save to Excel\n",
    "df_table.to_excel(\"extracted_table.xlsx\", index=False)\n",
    "\n",
    "print(\"Data extraction complete. Saved to extracted_table.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete. Saved to extracted_table.xlsx\n"
     ]
    }
   ],
   "source": [
    "######working\n",
    "\n",
    "import cv2\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image_path = '3.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print(f\"Error: Unable to load image at path {image_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Preprocess the image\n",
    "def preprocess_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Sharpen the image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharp = cv2.filter2D(gray, -1, kernel)\n",
    "    \n",
    "    # Denoise the image\n",
    "    denoised = cv2.fastNlMeansDenoising(sharp, h=30)\n",
    "    \n",
    "    # Resize the image\n",
    "    height, width = denoised.shape\n",
    "    resized = cv2.resize(denoised, (width * 3, height * 3))\n",
    "    \n",
    "    # Apply both adaptive and global thresholding\n",
    "    adaptive_thresh = cv2.adaptiveThreshold(resized, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                            cv2.THRESH_BINARY, 11, 2)\n",
    "    _, global_thresh = cv2.threshold(resized, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Combine both thresholding results\n",
    "    combined_thresh = cv2.bitwise_or(adaptive_thresh, global_thresh)\n",
    "    \n",
    "    return combined_thresh\n",
    "\n",
    "# Preprocess the image\n",
    "thresh = preprocess_image(image)\n",
    "\n",
    "# Use Tesseract to perform OCR with custom configurations\n",
    "custom_config = r'--oem 3 --psm 6'\n",
    "data = pytesseract.image_to_data(thresh, output_type=Output.DICT, config=custom_config)\n",
    "\n",
    "# Extract the OCR results into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter out non-table data\n",
    "table_data = df[df['text'].str.strip().astype(bool)]\n",
    "\n",
    "# Initialize variables to store row data\n",
    "rows = []\n",
    "current_row = []\n",
    "current_line_num = table_data['line_num'].min()\n",
    "\n",
    "# Iterate through the table data\n",
    "for i, row in table_data.iterrows():\n",
    "    if row['line_num'] != current_line_num:\n",
    "        if current_row:\n",
    "            rows.append(' '.join(current_row))\n",
    "        current_row = []\n",
    "        current_line_num = row['line_num']\n",
    "    \n",
    "    # Concatenate text within the same line\n",
    "    current_row.append(row['text'])\n",
    "\n",
    "# Append the last row\n",
    "if current_row:\n",
    "    rows.append(' '.join(current_row))\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "df_table = pd.DataFrame(rows)\n",
    "\n",
    "# Ensure DataFrame has columns and rename them if needed\n",
    "# Assuming the first row of `rows` is the header\n",
    "if not df_table.empty:\n",
    "    header = df_table.iloc[0]\n",
    "    df_table = df_table[1:]\n",
    "    df_table.columns = header\n",
    "\n",
    "# Save to Excel\n",
    "df_table.to_excel(\"extracted_table.xlsx\", index=False)\n",
    "\n",
    "print(\"Data extraction complete. Saved to extracted_table.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning and splitting complete. Saved to cleaned_extracted_table.xlsx\n"
     ]
    }
   ],
   "source": [
    "## working when O separated\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the previously extracted data from Excel\n",
    "df_table = pd.read_excel(\"extracted_table.xlsx\", header=None)\n",
    "\n",
    "# Function to clean and split lines where \"O \" is present\n",
    "def clean_and_split(line):\n",
    "    if isinstance(line, str) and line.startswith(\"O \"):\n",
    "        # Replace multiple spaces with a single space\n",
    "        line = re.sub(r'\\s+', ' ', line)\n",
    "        # Split the line by spaces\n",
    "        split_line = line.split(' ')\n",
    "        # Join the elements with semicolons\n",
    "        return ';'.join(split_line)\n",
    "    return line\n",
    "\n",
    "# Apply the cleaning function to each row in the DataFrame\n",
    "df_table_cleaned = df_table[0].apply(clean_and_split)\n",
    "\n",
    "# Split the semicolon-separated values into separate columns\n",
    "df_table_split = df_table_cleaned.str.split(';', expand=True)\n",
    "\n",
    "# Save the cleaned and split data to a new Excel file\n",
    "df_table_split.to_excel(\"cleaned_extracted_table.xlsx\", index=False)\n",
    "\n",
    "print(\"Data cleaning and splitting complete. Saved to cleaned_extracted_table.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning and splitting complete. Saved to cleaned_extracted_table.xlsx\n"
     ]
    }
   ],
   "source": [
    "###working \n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the previously extracted data from Excel\n",
    "df_table = pd.read_excel(\"extracted_table.xlsx\", header=None)\n",
    "\n",
    "# Function to clean and split lines where \"O \" is present\n",
    "def clean_and_split(line):\n",
    "    if isinstance(line, str) and line.startswith(\"O \"):\n",
    "        # Replace multiple spaces with a single space\n",
    "        line = re.sub(r'\\s+', ' ', line)\n",
    "        # Split the line by spaces\n",
    "        split_line = line.split(' ')\n",
    "        # Join the elements with semicolons\n",
    "        return ';'.join(split_line)\n",
    "    return line\n",
    "\n",
    "# Function to merge lines if a line might be a continuation of the previous line\n",
    "def merge_lines(data):\n",
    "    merged_data = []\n",
    "    for i, line in enumerate(data):\n",
    "        if i > 0:\n",
    "            # Check if the current line is a continuation of the previous line\n",
    "            if isinstance(line, str) and line.islower():\n",
    "                # Merge with the previous line\n",
    "                merged_data[-1] += \" \" + line.strip()\n",
    "            else:\n",
    "                merged_data.append(line)\n",
    "        else:\n",
    "            merged_data.append(line)\n",
    "    return merged_data\n",
    "\n",
    "# Apply the merge_lines function to combine split lines\n",
    "merged_table = merge_lines(df_table[0].tolist())\n",
    "\n",
    "# Apply the cleaning function to each row in the merged data\n",
    "df_table_cleaned = pd.Series(merged_table).apply(clean_and_split)\n",
    "\n",
    "# Split the semicolon-separated values into separate columns\n",
    "df_table_split = df_table_cleaned.str.split(';', expand=True)\n",
    "\n",
    "# Save the cleaned and split data to a new Excel file\n",
    "df_table_split.to_excel(\"cleaned_extracted_table.xlsx\", index=False)\n",
    "\n",
    "print(\"Data cleaning and splitting complete. Saved to cleaned_extracted_table.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete, excluding the first two rows. Saved to cleaned_extracted_table_remaining.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the previously extracted data from Excel\n",
    "df_table = pd.read_excel(\"extracted_table.xlsx\", header=None)\n",
    "\n",
    "# Function to clean and split lines where \"O \" is present\n",
    "def clean_and_split(line):\n",
    "    if isinstance(line, str) and line.startswith(\"O \"):\n",
    "        # Replace multiple spaces with a single space\n",
    "        line = re.sub(r'\\s+', ' ', line)\n",
    "        # Split the line by spaces\n",
    "        split_line = line.split(' ')\n",
    "        # Join the elements with semicolons\n",
    "        return ';'.join(split_line)\n",
    "    return line\n",
    "\n",
    "# Function to merge lines that are likely part of the same header or value\n",
    "def merge_lines(data):\n",
    "    merged_data = []\n",
    "    for i, line in enumerate(data):\n",
    "        if i > 0:\n",
    "            # Check if the current line is likely a continuation of the previous line\n",
    "            if isinstance(line, str) and (line.islower() or line.strip() == ''):\n",
    "                # Merge with the previous line\n",
    "                merged_data[-1] += \" \" + line.strip()\n",
    "            elif isinstance(data[i-1], str) and data[i-1].islower():\n",
    "                # Merge with the previous line if the previous line was lowercased\n",
    "                merged_data[-1] += \" \" + line.strip()\n",
    "            else:\n",
    "                merged_data.append(line)\n",
    "        else:\n",
    "            merged_data.append(line)\n",
    "    return merged_data\n",
    "\n",
    "# Skip the first two rows and extract the remaining data\n",
    "df_table_remaining = df_table.iloc[2:]\n",
    "\n",
    "# Apply the merge_lines function to combine split lines in the remaining data\n",
    "merged_table = merge_lines(df_table_remaining[0].tolist())\n",
    "\n",
    "# Apply the cleaning function to each row in the merged data\n",
    "df_table_cleaned = pd.Series(merged_table).apply(clean_and_split)\n",
    "\n",
    "# Split the semicolon-separated values into separate columns\n",
    "df_table_split = df_table_cleaned.str.split(';', expand=True)\n",
    "\n",
    "# Save the cleaned and split data to a new Excel file\n",
    "df_table_split.to_excel(\"cleaned_extracted_table_remaining.xlsx\", index=False)\n",
    "\n",
    "print(\"Data extraction complete, excluding the first two rows. Saved to cleaned_extracted_table_remaining.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and splitting complete. Saved to cleaned_extracted_table_remaining_corrected.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the previously extracted data from Excel\n",
    "df_table = pd.read_excel(\"extracted_table.xlsx\", header=None)\n",
    "\n",
    "# Skip the first two rows and extract the remaining data\n",
    "df_table_remaining = df_table.iloc[2:]\n",
    "\n",
    "# Convert the DataFrame to a single string to process all rows together\n",
    "all_text = ' '.join(df_table_remaining[0].dropna().astype(str).tolist())\n",
    "\n",
    "# Split the text by \" O \" followed by a space, but keep the delimiter in the result\n",
    "rows = re.split(r'(O )', all_text)\n",
    "rows = [''.join(pair) for pair in zip(rows[1::2], rows[2::2])]\n",
    "\n",
    "# Process each row to split by spaces and clean\n",
    "def process_row(row):\n",
    "    row = re.sub(r'\\s+', ' ', row).strip()  # Replace multiple spaces with one\n",
    "    return row.split(' ')\n",
    "\n",
    "# Apply the processing to each row\n",
    "processed_data = [process_row(row) for row in rows]\n",
    "\n",
    "# Convert the processed data into a DataFrame\n",
    "df_cleaned = pd.DataFrame(processed_data)\n",
    "\n",
    "# Save the cleaned and split data to a new Excel file\n",
    "df_cleaned.to_excel(\"cleaned_extracted_table_remaining_corrected.xlsx\", index=False)\n",
    "\n",
    "print(\"Data extraction and splitting complete. Saved to cleaned_extracted_table_remaining_corrected.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and splitting complete. Saved to cleaned_extracted_table_remaining_corrected.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the previously extracted data from Excel\n",
    "df_table = pd.read_excel(\"extracted_table.xlsx\", header=None)\n",
    "\n",
    "# Skip the first two rows and extract the remaining data\n",
    "df_table_remaining = df_table.iloc[2:]\n",
    "\n",
    "# Convert the DataFrame to a single string to process all rows together\n",
    "all_text = ' '.join(df_table_remaining[0].dropna().astype(str).tolist())\n",
    "\n",
    "# Split the text by \"O \" followed by a space, but keep the delimiter in the result\n",
    "rows = re.split(r'(O )', all_text)\n",
    "rows = [''.join(pair) for pair in zip(rows[1::2], rows[2::2])]\n",
    "\n",
    "# Function to process each row, handling sentences as single units\n",
    "def process_row(row):\n",
    "    # The pattern below captures groups of text split by spaces, except when the text is inside quotes.\n",
    "    pattern = re.compile(r'\".+?\"|[^\"\\s]+')\n",
    "    matches = pattern.findall(row)\n",
    "\n",
    "    # Strip quotes from the matched sentences\n",
    "    cleaned_matches = [match.strip('\"') for match in matches]\n",
    "    \n",
    "    return cleaned_matches\n",
    "\n",
    "# Apply the processing to each row\n",
    "processed_data = [process_row(row) for row in rows]\n",
    "\n",
    "# Convert the processed data into a DataFrame\n",
    "df_cleaned = pd.DataFrame(processed_data)\n",
    "\n",
    "# Save the cleaned and split data to a new Excel file\n",
    "df_cleaned.to_excel(\"cleaned_extracted_table_remaining_corrected.xlsx\", index=False, header=False)\n",
    "\n",
    "print(\"Data extraction and splitting complete. Saved to cleaned_extracted_table_remaining_corrected.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 7 elements, new values have 4 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14540\\645737055.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;31m# Setting the headers manually based on expected columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'age'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'remote worker'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mdf_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdf_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m# Save to Excel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5586\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5587\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5588\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5589\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5590\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    767\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    770\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;31m# Caller is responsible for ensuring we have an Index object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\base.py\u001b[0m in \u001b[0;36m_validate_set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m     70\u001b[0m                 \u001b[1;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;34mf\"values have {new_len} elements\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 7 elements, new values have 4 elements"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the image\n",
    "image_path = 'sample_image.png'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Preprocess the image\n",
    "def preprocess_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Sharpen the image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharp = cv2.filter2D(gray, -1, kernel)\n",
    "    \n",
    "    # Resize the image\n",
    "    height, width = sharp.shape\n",
    "    sharp = cv2.resize(sharp, (width*2, height*2))\n",
    "    \n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(sharp, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                   cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    return thresh\n",
    "\n",
    "# Preprocess the image\n",
    "thresh = preprocess_image(image)\n",
    "\n",
    "# Use Tesseract to perform OCR with custom configurations\n",
    "custom_config = r'--oem 3 --psm 6'\n",
    "data = pytesseract.image_to_data(thresh, output_type=Output.DICT, config=custom_config)\n",
    "\n",
    "# Extract the OCR results into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter out non-table data\n",
    "table_data = df[df['text'].str.strip().astype(bool)]\n",
    "\n",
    "# Initialize variables to store row data\n",
    "rows = []\n",
    "current_row = []\n",
    "current_line_num = table_data['line_num'].min()\n",
    "\n",
    "# Iterate through the table data\n",
    "for i, row in table_data.iterrows():\n",
    "    if row['line_num'] != current_line_num:\n",
    "        if current_row:\n",
    "            rows.append(' '.join(current_row))\n",
    "        current_row = []\n",
    "        current_line_num = row['line_num']\n",
    "    \n",
    "    # Concatenate text within the same line\n",
    "    current_row.append(row['text'])\n",
    "\n",
    "# Append the last row\n",
    "if current_row:\n",
    "    rows.append(' '.join(current_row))\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "df_table = pd.DataFrame(rows)\n",
    "\n",
    "# Manually set the header and split rows into columns\n",
    "if not df_table.empty:\n",
    "    df_table.columns = ['row']\n",
    "    df_table = df_table['row'].str.split(expand=True)\n",
    "    \n",
    "    # Setting the headers manually based on expected columns\n",
    "    headers = ['name', 'age', 'position', 'remote worker']\n",
    "    df_table.columns = headers[:df_table.shape[1]]\n",
    "\n",
    "# Save to Excel\n",
    "df_table.to_excel(\"extracted_table.xlsx\", index=False)\n",
    "\n",
    "print(\"Data extraction complete. Saved to extracted_table.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete. Saved to extracted_table_no_header.xlsx\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image_path = 'sample_image.png'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Preprocess the image\n",
    "def preprocess_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Sharpen the image\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharp = cv2.filter2D(gray, -1, kernel)\n",
    "    \n",
    "    # Resize the image\n",
    "    height, width = sharp.shape\n",
    "    sharp = cv2.resize(sharp, (width*2, height*2))\n",
    "    \n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(sharp, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                   cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    return thresh\n",
    "\n",
    "# Preprocess the image\n",
    "thresh = preprocess_image(image)\n",
    "\n",
    "# Use Tesseract to perform OCR with custom configurations\n",
    "custom_config = r'--oem 3 --psm 6'\n",
    "data = pytesseract.image_to_data(thresh, output_type=Output.DICT, config=custom_config)\n",
    "\n",
    "# Extract the OCR results into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter out non-table data\n",
    "table_data = df[df['text'].str.strip().astype(bool)]\n",
    "\n",
    "# Initialize variables to store row data\n",
    "rows = []\n",
    "current_row = []\n",
    "current_line_num = table_data['line_num'].min()\n",
    "\n",
    "# Iterate through the table data\n",
    "for i, row in table_data.iterrows():\n",
    "    if row['line_num'] != current_line_num:\n",
    "        if current_row:\n",
    "            rows.append(' '.join(current_row))\n",
    "        current_row = []\n",
    "        current_line_num = row['line_num']\n",
    "    \n",
    "    # Concatenate text within the same line\n",
    "    current_row.append(row['text'])\n",
    "\n",
    "# Append the last row\n",
    "if current_row:\n",
    "    rows.append(' '.join(current_row))\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "df_table = pd.DataFrame(rows)\n",
    "\n",
    "# Save to Excel without headers\n",
    "df_table.to_excel(\"extracted_table_no_header.xlsx\", index=False, header=False)\n",
    "\n",
    "print(\"Data extraction complete. Saved to extracted_table_no_header.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
